#!/usr/bin/env python3
"""
TechWatchIT - Syst√®me d'automatisation et de planification
Ex√©cution automatique des t√¢ches de veille, analyse et g√©n√©ration de rapports
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import schedule
import time
import threading
from datetime import datetime, timedelta
import logging
import subprocess

from src.fetch_feeds import FeedFetcher
from src.classifier import ArticleClassifier
from src.summarizer import ArticleSummarizer
from src.advanced_summarizer import AdvancedSummarizer
from src.database import DatabaseManager
from config.config import Config
import pymysql

class TechWatchITAutomation:
    """Syst√®me d'automatisation compl√®te de TechWatchIT"""
    
    def __init__(self):
        self.logger = Config.setup_logging()
        self.db = DatabaseManager()
        self.fetcher = FeedFetcher()
        self.classifier = ArticleClassifier()
        self.summarizer = ArticleSummarizer()
        self.advanced_summarizer = AdvancedSummarizer()
        self.running = False
        
        self.logger.info("ü§ñ Syst√®me d'automatisation TechWatchIT initialis√©")
    
    def setup_schedules(self):
        """Configurer les t√¢ches planifi√©es"""
        
        # R√©cup√©ration des flux RSS - Toutes les 30 minutes
        schedule.every(30).minutes.do(self.fetch_feeds_job)
        
        # Traitement IA des nouveaux articles - Toutes les heures
        schedule.every().hour.do(self.process_new_articles_job)
        
        # G√©n√©ration d'analyses d√©taill√©es - Toutes les 2 heures
        schedule.every(2).hours.do(self.generate_detailed_analyses_job)
        
        # Rapport quotidien - Tous les jours √† 8h00
        schedule.every().day.at("08:00").do(self.daily_report_job)
        
        # Nettoyage de la base - Tous les dimanches √† 02:00
        schedule.every().sunday.at("02:00").do(self.database_maintenance_job)
        
        # Sauvegarde - Tous les jours √† 23:00
        schedule.every().day.at("23:00").do(self.backup_job)
        
        self.logger.info("üìÖ Planification automatique configur√©e:")
        self.logger.info("   - R√©cup√©ration RSS: Toutes les 30 minutes")
        self.logger.info("   - Traitement IA: Toutes les heures")
        self.logger.info("   - Analyses d√©taill√©es: Toutes les 2 heures")
        self.logger.info("   - Rapport quotidien: 8h00")
        self.logger.info("   - Maintenance: Dimanche 2h00")
        self.logger.info("   - Sauvegarde: 23h00")
    
    def fetch_feeds_job(self):
        """T√¢che de r√©cup√©ration des flux RSS"""
        try:
            self.logger.info("üîÑ D√©but r√©cup√©ration automatique des flux RSS")
            feed_results = self.fetcher.fetch_all_feeds()
            self.logger.info(f"‚úÖ R√©cup√©ration termin√©e: {feed_results} articles trait√©s")
            
            # Notifier si articles critiques d√©tect√©s
            if feed_results.get('total_articles', 0) > 0:
                self._check_critical_articles()
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur r√©cup√©ration automatique: {str(e)}")
    
    def process_new_articles_job(self):
        """T√¢che de traitement IA des nouveaux articles"""
        try:
            self.logger.info("ü§ñ D√©but traitement IA automatique")
            
            # R√©cup√©rer les articles non trait√©s
            with self.db.get_connection() as conn:
                cursor = conn.cursor(pymysql.cursors.DictCursor)
                cursor.execute('''
                SELECT r.* FROM raw_articles r
                LEFT JOIN processed_articles p ON r.id = p.raw_article_id
                WHERE p.id IS NULL
                ORDER BY r.created_at DESC
                LIMIT 50
                ''')
                raw_articles = cursor.fetchall()
            
            if not raw_articles:
                self.logger.info("üìù Aucun nouvel article √† traiter")
                return
            
            processed_count = 0
            critical_count = 0
            
            for article in raw_articles:
                try:
                    # Classification
                    classification = self.classifier.classify_article(article)
                    
                    # R√©sum√© standard
                    summary = self.summarizer.summarize_article(article, classification)
                    
                    # Fusionner les donn√©es
                    processed_data = {**classification, **summary}
                    
                    # Sauvegarder
                    if self.db.save_processed_article(article['id'], processed_data):
                        processed_count += 1
                        
                        # Compter les articles critiques
                        if classification.get('severity_level') == 'critical':
                            critical_count += 1
                
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur traitement article {article['id']}: {str(e)}")
                    continue
            
            self.logger.info(f"‚úÖ Traitement IA termin√©: {processed_count} articles trait√©s")
            
            if critical_count > 0:
                self.logger.warning(f"‚ö†Ô∏è {critical_count} articles critiques d√©tect√©s - Analyse urgente recommand√©e")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur traitement IA automatique: {str(e)}")
    
    def generate_detailed_analyses_job(self):
        """T√¢che de g√©n√©ration d'analyses d√©taill√©es pour les articles critiques"""
        try:
            self.logger.info("üéØ D√©but g√©n√©ration analyses d√©taill√©es automatique")
            
            # R√©cup√©rer les articles critiques sans analyse d√©taill√©e
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute('''
                SELECT pa.*, ra.title, ra.content, ra.description, ra.link, ra.published_date
                FROM processed_articles pa
                INNER JOIN raw_articles ra ON pa.raw_article_id = ra.id
                WHERE pa.severity_level = 'critical' 
                AND pa.processed_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                AND (pa.detailed_analysis IS NULL OR pa.detailed_analysis = '')
                ORDER BY pa.processed_at DESC
                LIMIT 10
                ''')
                critical_articles = cursor.fetchall()
            
            if not critical_articles:
                self.logger.info("üìù Aucun article critique n√©cessitant une analyse d√©taill√©e")
                return
            
            analyses_generated = 0
            
            for article_data in critical_articles:
                try:
                    # Pr√©parer les donn√©es
                    article = {
                        'title': article_data['title'],
                        'content': article_data['content'],
                        'description': article_data['description'] or '',
                        'link': article_data['link'],
                        'published_date': article_data['published_date']
                    }
                    
                    classification = {
                        'category': article_data['category'],
                        'technology': article_data['technology'],
                        'severity_level': article_data['severity_level'],
                        'severity_score': article_data['severity_score'],
                        'cvss_score': article_data['cvss_score'],
                        'is_security_alert': article_data['is_security_alert']
                    }
                    
                    # G√©n√©rer l'analyse d√©taill√©e
                    detailed_analysis = self.advanced_summarizer.generate_detailed_analysis(article, classification)
                    
                    # Sauvegarder l'analyse d√©taill√©e
                    if detailed_analysis and detailed_analysis.get('detailed_summary'):
                        with self.db.get_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute('''
                            UPDATE processed_articles 
                            SET detailed_analysis = %s,
                                blog_article = %s,
                                executive_summary = %s,
                                technical_insights = %s
                            WHERE id = %s
                            ''', (
                                detailed_analysis.get('detailed_summary', ''),
                                detailed_analysis.get('blog_article', ''),
                                detailed_analysis.get('executive_summary', ''),
                                str(detailed_analysis.get('technical_insights', {})),
                                article_data['id']
                            ))
                            conn.commit()
                        
                        analyses_generated += 1
                        self.logger.info(f"üìä Analyse d√©taill√©e g√©n√©r√©e pour: {article['title'][:50]}...")
                
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur analyse d√©taill√©e article {article_data['id']}: {str(e)}")
                    continue
            
            self.logger.info(f"‚úÖ Analyses d√©taill√©es g√©n√©r√©es: {analyses_generated}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration analyses d√©taill√©es: {str(e)}")
    
    def daily_report_job(self):
        """T√¢che de g√©n√©ration du rapport quotidien"""
        try:
            self.logger.info("üìä G√©n√©ration du rapport quotidien automatique")
            
            # G√©n√©rer le rapport quotidien
            report_content = self._generate_daily_report()
            
            # Sauvegarder le rapport
            report_filename = f"rapport_quotidien_{datetime.now().strftime('%Y%m%d')}.txt"
            report_path = os.path.join("logs", report_filename)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"‚úÖ Rapport quotidien sauvegard√©: {report_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration rapport quotidien: {str(e)}")
    
    def database_maintenance_job(self):
        """T√¢che de maintenance de la base de donn√©es"""
        try:
            self.logger.info("üßπ D√©but maintenance automatique de la base")
            
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                # Supprimer les anciens articles (> 90 jours)
                cursor.execute('''
                DELETE FROM raw_articles 
                WHERE created_at < DATE_SUB(NOW(), INTERVAL 90 DAY)
                ''')
                deleted_raw = cursor.rowcount
                
                # Optimiser les tables
                cursor.execute("OPTIMIZE TABLE raw_articles")
                cursor.execute("OPTIMIZE TABLE processed_articles")
                
                conn.commit()
            
            self.logger.info(f"‚úÖ Maintenance termin√©e: {deleted_raw} anciens articles supprim√©s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur maintenance base: {str(e)}")
    
    def backup_job(self):
        """T√¢che de sauvegarde automatique"""
        try:
            self.logger.info("üíæ D√©but sauvegarde automatique")
            
            backup_filename = f"techwatchit_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
            backup_path = os.path.join("logs", backup_filename)
            
            # Commande mysqldump
            cmd = [
                "mysqldump",
                "-h", "localhost",
                "-u", "root",
                "--single-transaction",
                "--routines",
                "--triggers",
                "techwatchit"
            ]
            
            with open(backup_path, 'w') as f:
                subprocess.run(cmd, stdout=f, check=True)
            
            self.logger.info(f"‚úÖ Sauvegarde termin√©e: {backup_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde: {str(e)}")
    
    def _check_critical_articles(self):
        """V√©rifier s'il y a de nouveaux articles critiques"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute('''
                SELECT COUNT(*) as count
                FROM processed_articles 
                WHERE severity_level = 'critical' 
                AND processed_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
                ''')
                result = cursor.fetchone()
                
                if result and result['count'] > 0:
                    self.logger.warning(f"üö® ALERTE: {result['count']} nouveaux articles critiques d√©tect√©s")
                    
        except Exception as e:
            self.logger.error(f"‚ùå Erreur v√©rification articles critiques: {str(e)}")
    
    def _generate_daily_report(self) -> str:
        """G√©n√©rer le contenu du rapport quotidien"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                # Statistiques du jour
                cursor.execute('''
                SELECT 
                    COUNT(*) as total_articles,
                    SUM(CASE WHEN severity_level = 'critical' THEN 1 ELSE 0 END) as critical_count,
                    SUM(CASE WHEN severity_level = 'high' THEN 1 ELSE 0 END) as high_count,
                    SUM(CASE WHEN is_security_alert = 1 THEN 1 ELSE 0 END) as security_alerts
                FROM processed_articles 
                WHERE DATE(processed_at) = CURDATE()
                ''')
                stats = cursor.fetchone()
                
                # Articles critiques du jour
                cursor.execute('''
                SELECT pa.technology, ra.title, pa.severity_score
                FROM processed_articles pa
                INNER JOIN raw_articles ra ON pa.raw_article_id = ra.id
                WHERE pa.severity_level = 'critical' 
                AND DATE(pa.processed_at) = CURDATE()
                ORDER BY pa.severity_score DESC
                LIMIT 5
                ''')
                critical_articles = cursor.fetchall()
                
                # G√©n√©rer le rapport
                report = f"""
RAPPORT QUOTIDIEN TECHWATCHIT
========================================
Date: {datetime.now().strftime('%d/%m/%Y %H:%M')}

STATISTIQUES DU JOUR
--------------------
üìä Total articles trait√©s: {stats['total_articles'] if stats else 0}
üö® Articles critiques: {stats['critical_count'] if stats else 0}
‚ö†Ô∏è Articles haute s√©v√©rit√©: {stats['high_count'] if stats else 0}
üõ°Ô∏è Alertes s√©curit√©: {stats['security_alerts'] if stats else 0}

ARTICLES CRITIQUES DU JOUR
---------------------------
"""
                
                if critical_articles:
                    for i, article in enumerate(critical_articles, 1):
                        report += f"{i}. [{article['technology'].upper()}] {article['title']} (Score: {article['severity_score']}/10)\n"
                else:
                    report += "Aucun article critique aujourd'hui.\n"
                
                report += f"""
========================================
Rapport g√©n√©r√© automatiquement par TechWatchIT
Syst√®me de veille technologique automatis√©
"""
                
                return report
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration rapport: {str(e)}")
            return f"Erreur g√©n√©ration rapport: {str(e)}"
    
    def start_automation(self):
        """D√©marrer le syst√®me d'automatisation"""
        self.running = True
        self.setup_schedules()
        
        self.logger.info("üöÄ Syst√®me d'automatisation TechWatchIT d√©marr√©")
        self.logger.info("‚è∞ Planificateur en cours d'ex√©cution...")
        
        try:
            while self.running:
                schedule.run_pending()
                time.sleep(60)  # V√©rifier toutes les minutes
                
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è Arr√™t du syst√®me d'automatisation demand√©")
            self.stop_automation()
    
    def stop_automation(self):
        """Arr√™ter le syst√®me d'automatisation"""
        self.running = False
        schedule.clear()
        self.logger.info("üõë Syst√®me d'automatisation TechWatchIT arr√™t√©")
    
    def run_manual_task(self, task_name: str):
        """Ex√©cuter manuellement une t√¢che sp√©cifique"""
        tasks = {
            'fetch': self.fetch_feeds_job,
            'process': self.process_new_articles_job,
            'analyze': self.generate_detailed_analyses_job,
            'report': self.daily_report_job,
            'maintenance': self.database_maintenance_job,
            'backup': self.backup_job
        }
        
        if task_name in tasks:
            self.logger.info(f"üîß Ex√©cution manuelle de la t√¢che: {task_name}")
            tasks[task_name]()
        else:
            self.logger.error(f"‚ùå T√¢che inconnue: {task_name}")
            self.logger.info(f"üìã T√¢ches disponibles: {', '.join(tasks.keys())}")

def main():
    """Point d'entr√©e principal"""
    automation = TechWatchITAutomation()
    
    if len(sys.argv) > 1:
        # Mode manuel
        task = sys.argv[1]
        automation.run_manual_task(task)
    else:
        # Mode automatique
        try:
            automation.start_automation()
        except KeyboardInterrupt:
            print("\nüõë Arr√™t demand√© par l'utilisateur")

if __name__ == "__main__":
    main() 